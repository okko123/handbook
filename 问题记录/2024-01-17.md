- 问题1，表现：locallife-web容器更新后，ip地址发生改变后。访问出现504超时。
- 问题排查：
  1. 检查kong-proxy是否正常运行；检查端口是否开发，使用curl -H 带host头访问不同域名。只有locallife的域名出现504。
  2. 检查kong-proxy的访问日志，发现kong-proxy仍旧把请求转发到locallife-web容器的旧IP上，实际上旧IP的容器已经不存在，导致出现504超时的问题
  3. 检查kong-admin报错，发现报错信息。检查locallife的k8s的ep是不是丢失。只有在容器更新的时候，ep的地址是会消失，容器更新后，ep地址恢复。因此不是故障点
     ```bash
     time="2024-01-10T06:54:11Z" level=error msg="resource processing failed: can't add target for backend saas-web-8080: no kubernetes service found" GVK="networking.k8s.io/v1, Kind=Ingress" name=sass-web-ingress namespace=stage
     time="2024-01-10T06:54:11Z" level=info msg="no targets found to create upstream" service_name=stage.sass-web-ingress.saas-web-8080.80
     time="2024-01-10T06:54:14Z" level=error msg="failed to fetch service: Service stage/saas-web-8080 not found" service_name= service_namespace=stage
     ```
  4. 使用konga检查kong-admin的配置信息，发现konga无法访问
  5. 检查konga连接的pg数据库，发现pg数据库已经停止，重启pg数据库。恢复konga访问；因为kong-admin与konga使用同一个pg实例
  6. 再次更新localfile容器实例，刷新IP地址。kong-proxy能正确获取upstream信息。访问恢复正常
- 分析：
  > 因为pg实例关闭，导致kong-admin无法将pods容器更新的IP信息写入数据库，导致kong-proxy无法正常获取IP，更新upstream信息
---
- 问题2，表现k8s集群新创建的容器无法访问自身容器外的任何ip地址
  1. 系统版本：k8s-1.18.20
- 问题排查：
  1. 使用busybox，在所有节点上创建容器，登陆busybox容器，ping 公网IP、ping 宿主IP，发现部分节点出现问题。已有的容器没有网络问题
  2. 将部分问题节点设置为不可调度的状态（cordon），添加新的worker节点，当容器重启后自动迁移至新的worker节点
  3. 将节点上的容器迁移后，重置节点后，再加入集群，问题解决
  4. 再进行检查；创建容器后，在宿主上执行ip route show，发现容器对应的路由信息没有生成
  5. 检查节点上的calico-node容器日志，发现同步数据失败
     ```bash
     2024-01-16 03:36:35.990 [INFO][386944] felix/watchercache.go 175: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/networksets"
     2024-01-16 03:36:36.001 [INFO][59] confd/watchercache.go 175: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/ippools"
     2024-01-16 03:36:36.002 [INFO][59] confd/watchercache.go 175: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/bgppeers"
     2024-01-16 03:36:36.003 [INFO][59] confd/watchercache.go 188: Failed to perform list of current data during resync ListRoot="/calico/resources/v3/projectcalico.org/ippools" error=The resourceVersion for the provided list is too old.
     2024-01-16 03:36:36.004 [INFO][59] confd/watchercache.go 188: Failed to perform list of current data during resync ListRoot="/calico/resources/v3/projectcalico.org/bgppeers" error=The resourceVersion for the provided list is too old.
     2024-01-16 03:36:36.110 [INFO][60] status-reporter/watchercache.go 175: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/caliconodestatuses"
     2024-01-16 03:36:36.112 [INFO][60] status-reporter/watchercache.go 188: Failed to perform list of current data during resync ListRoot="/calico/resources/v3/projectcalico.org/caliconodestatuses" error=The resourceVersion for the provided list is too old.
     2024-01-16 03:36:36.189 [INFO][386944] felix/watchercache.go 175: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/hostendpoints"
     2024-01-16 03:36:36.189 [INFO][386944] felix/watchercache.go 188: Failed to perform list of current data during resync ListRoot="/calico/resources/v3/projectcalico.org/ippools" error=The resourceVersion for the provided list is too old.
     2024-01-16 03:36:36.389 [INFO][386944] felix/watchercache.go 188: Failed to perform list of current data during resync ListRoot="/calico/ipam/v2/assignment/" error=The resourceVersion for the provided list is too old.
     2024-01-16 03:36:36.390 [INFO][386944] felix/watchercache.go 175: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/networkpolicies"
     2024-01-16 03:36:36.445 [INFO][59] confd/watchercache.go 175: Full resync is required ListRoot="/calico/ipam/v2/host/cnhqvztk8sm03"
     2024-01-16 03:36:36.445 [INFO][386944] felix/watchercache.go 175: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/workloadendpoints"
     2024-01-16 03:36:36.446 [INFO][386944] felix/watchercache.go 175: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/profiles"
     2024-01-16 03:36:36.447 [INFO][386944] felix/watchercache.go 188: Failed to perform list of current data during resync ListRoot="/calico/resources/v3/projectcalico.org/workloadendpoints" error=The resourceVersion for the provided list is too old.
     2024-01-16 03:36:36.447 [INFO][59] confd/watchercache.go 188: Failed to perform list of current data during resync ListRoot="/calico/ipam/v2/host/cnhqvztk8sm03" error=The resourceVersion for the provided list is too old.
     2024-01-16 03:36:36.448 [INFO][386944] felix/watchercache.go 175: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/kubernetesendpointslices"
     2024-01-16 03:36:36.448 [INFO][386944] felix/watchercache.go 188: Failed to perform list of current data during resync ListRoot="/calico/resources/v3/projectcalico.org/profiles" error=The resourceVersion for the provided list is too old.
     2024-01-16 03:36:36.449 [INFO][386944] felix/watchercache.go 175: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/kubernetesnetworkpolicies"
     2024-01-16 03:36:36.449 [INFO][386944] felix/watchercache.go 188: Failed to perform list of current data during resync ListRoot="/calico/resources/v3/projectcalico.org/kubernetesendpointslices" error=The resourceVersion for the provided list is too old.
     2024-01-16 03:36:36.450 [INFO][386944] felix/watchercache.go 188: Failed to perform list of current data during resync ListRoot="/calico/resources/v3/projectcalico.org/kubernetesnetworkpolicies" error=The resourceVersion for the provided list is too old.
     2024-01-16 03:36:36.589 [INFO][386944] felix/watchercache.go 188: Failed to perform list of current data during resync ListRoot="/calico/resources/v3/projectcalico.org/globalnetworkpolicies" error=The resourceVersion for the provided list is too old.
     2024-01-16 03:36:36.590 [INFO][386944] felix/watchercache.go 175: Full resync is required ListRoot="/calico/resources/v3/projectcalico.org/clusterinformations"
     ```
  6. 重启节点上的calico-node容器，问题解决
- 问题分析:
  > 可能因为在apiserver上添加--enable-aggregator-routing=true，导致问题出现
---
- 问题3：
  - 执行kubectl top nodes出现
    ```bash
    Error from server (ServiceUnavailable): the server is currently unable to handle the request (get pods.metrics.k8s.io)
    ```
  - apiserver出现的错误信息
    ```bash
    E0826 04:25:46.764318       1 available_controller.go:420] v1beta1.metrics.k8s.io failed with: failing or missing response from https://10.108.243.54:443/apis/metrics.k8s.io/v1beta1: Get https://10.108.243.54:443/apis/metrics.k8s.io/v1beta1: net/http: request canceled while waiting for connection (Client.Timeout exceeded while awaiting headers)
    ```
  - prometheus监控报警，KubeClientCertificateExpiration，客户端证书过期，但使用kubeadm检查集群证书没有过期
- 问题排查：
  1. 重启apiserver、kube-controller-manager、kube-scheduler
  2. 在apiserver的yaml配置文件，添加参数--enable-aggregator-routing=true，重启容器。问题解决
     - 启用API Aggregator，API Aggregation 允许在不修改 Kubernetes 核心代码的同时扩展 Kubernetes API，即：将第三方服务注册到 Kubernetes API 中，这样就可以通过 Kubernetes API 来访问第三方服务了，例如：Metrics Server API。注：另外一种扩展 Kubernetes API 的方法是使用 CRD（Custom Resource Definition，自定义资源定义）。