## kubernetes更新coreDNS配置，导致内部DNS无法使用
### 基础信息
- K8S版本：1.18.8
- calico版本：3.16.4
- kube-proxy，使用ipvs模式
### 问题描述
1. 更新coreDNS的configmap配置
2. 执行kubectl rollout restart -n kube-system deployment/coredns，对coreDNS容器进行滚动更新
3. 执行dig @10.96.0.10 www.baidu.com +short检查解析，发现超时，无法连接。
   ```bash
   dig @Service-IP www.baidu.com +short
    
   结果如下：
   dig @10.96.0.10 www.163.com +short
    
   ; <<>> DiG 9.11.13-RedHat-9.11.13-3.el8 <<>> @10.96.0.10 www.163.com +short
   ; (1 server found)
   ;; global options: +cmd
   ;; connection timed out; no servers could be reached
   ```
4. 执行dig @CoreDNS-Pod-IP www.baidu.com +short，正常
   ```bash
   dig @Pod-IP www.baidu.com +short
    
   正常的结果如下：
   dig @10.244.202.11 www.baidu.com +short
   www.a.shifen.com.
   14.215.177.39
   14.215.177.38
   ```
### 排查问题
1. 还原配置，并重启coreDNS。无效
2. 在每个节点上的容器，执行ping www.baidu.com的命令。发现前10台主机出现无法解析的问题，后面5台主机解析正常
3. 在宿主机上，调用容器的coreDNS解析（确认calico网络是否出现故障、确认coreDNS是否正常工作）。结果正常
4. 在宿主机上，访问service的IP，调用coreDNS解析，发现请求失败。确认service没有将流量转发至后端coreDNS的Pod上
5. 使用coreDNS的svc配置，新建svc，通过新的svc的地址，能正常将请求转发至后端的Pod上
6. 对比正常主机与异常的主机上的iptables的nat表，发现正常机器上，KUBE-SERVICE链上没有关于53端口的规则
7. 在问题主机上将nat表KUBE-SERVICE链上的有关53端口的规则删除，恢复正常
### 原因
1. 由于启用ipvs模式后，kube-proxy不处理iptables上现有的规则，导致dns的请求仍然被旧iptables捕获，并转发至旧的Pod IP上，导致DNS解析失败。通过删除iptables规则即可恢复
### 解决方法
1. 重启节点，kube-proxy会自动初始化iptables规则
2. calico 3.16，不会自动初始化iptables规则
3. 使用新版版本的calico，3.19.1，清理所有iptables规则后，能自动初始化iptables规则